Problem
=======

One drawback of Erlang is that it is just so flexible.  While this is good
for a coder that needs a powerful environment, it generates unwanted
operational load for those who have to deploy applications.  Recently, there
has been a movement among operations-concerned developers towards convention-
over-configuration as a way to manage this complexity.  erlctl is an attempt
to bring these benefits to the deployment and management of Erlang
applications.

Best Practices
==============

After a survey of major Erlang applications, the consensus seems to be that
the node should be named the same as the application for the common case of
deploying one copy of an application at a node.  Usually an option is provided
to indicate if another node is to be managed.

Distributed Erlang is usually used to communicate commands to a running node.
Most applications seem to use the Erlang convention of using $HOME to find a
cookie file for authentication.

Some applications use Unix symlinks to allow one program to serve multiple
purposes.  Examples include bzip2, lzma, busybox, and mmv.

All of this is complicated by the fact that Erlang includes a powerful, but
unwieldy, application deployment paradigm.

Architecture
============

With this in mind, erlctl has been designed to give a simple, comprehensive
convention for deploying and controlling Erlang applications.  It consists of
two components: the command line tool itself, and an application to run inside
of Erlang nodes to be controlled.

Command Line Interface
======================

The format for using the erlctl command is:

  erlctl [<system options>] [<application>:]<command> [args ...]

Alternatively, the command may be linked from another name.  In this case,
the usage becomes:

  <application>[[_]ctl] [<systems options>] <command> [args ...]

The system options are:

  -v
    Provides information messages from the internals of erlctl.
    This is mostly useful for debugging broken networking configurations.
  -h <host name of machine>
    Specifies which Erlang hostname should be used.
    Default determined using inet:gethostname/0.
  -l | -s
    Determines whether longnames or shortnames will be used for distribution.
    Defaults to shortnames.
  -n <local part of node name>
    Specifies the local portion of the target VM's node name
    Defaults to <application>
  -N <full node name>
    Specified the whole node name of the target VM
    Defaults to <application>@<hostname>
  -c <config file>
    Specifies a configuration file.  This file can be managed with the config
    commands or with your text editor of choice.
    Defaults to $HOME/.application.config
    FIXME:  Config files aren't implemented yet!

Application Interface
=====================

The erlctl application should be running in each node to be managed. Your
application must depend on erlctl or call erlctl:start/0 if it doesn't use the
application system. It should also call erlctl:register(appname) when it is
ready to be managed at runtime (where appname is the same atom name as your
application is named).

It should provide a module called appname_ctl. This module should provide the
interface functions to implement various actions. All functions should expect
three arguments: Context, Config, and Args.  NOTE: This module must be 
available on the client system as well as the server system.

When a command is executed, an Erlang node starts in the CLI command and there
may also be an Erlang node already running for that application.

The correct command implementation function is executed based on the context
and the system state. If you provide the correct commands, most use-cases
should be manageable. For commands that should always just be executed in the
CLI process, the 'always' context is provided. For commands that should be
executed in a running server context, the 'running' context is there. To
provide fallback behavior for commands in the 'running' context, there is a
'not_running' context. Finally, when a node is 'not_running', it can request
to start a node, which will then execute the command in the 'started' context.

The arguments are the remainder of the command line as split by your shell, as
strings.

The return value of command implementation functions is examined as followed:

 ok:                   if it hasn't exited, causes script to exit with success
 skip:                  skips this clause, as if it hadn't been matched
 {ok,Msg}:              same as ok with message
 {ok,Fmt,Data}:         same as ok with formatted message of data
 error:     if it hasn't exited, causes script to exit with failure (code 255)
 {error,N}:             same as error, with error code N
 {error,N,Msg}:         same as {error,N}, with message
 {error,N,Fmt,Data}:    same as {error,N}, with formatted message of data
 start:                 starts a VM
 {start,Opts}:          same as start, with a list of VM options
 {start,Opts,Msg}:      same as {start,Opts} with a message
 {start,Opts,Msg,Data}: same as {start,Opts} with a formatted message of data
 restart:               attempts to start after ensuring the node died
 {restart,Opts}:        as restart, but attempts {start,Opts}
 {restart,Opts,M}:      same as restart, but with a message
 {restart,Opts,M,D}:    same as restart, but with a formatted message
 Anything Else:         like {error,254}, and a warning message is emitted

NOTE: For restarts, the restart command stop the VM itself, just as a stop
      command would (i.e. erlctl:server_exit/0).

If no clause matches for a certain context, this is considered the same as a
return of 'skip'.  If an exception, error, or exit is triggered, it is
considered an error, and an appropriate error response (with code 254 and a
message indicating the exception).

Context Selection
=================

The process for selecting Context has a fair bit of voodoo to achieve the
behavior that I feel is useful. Roughly, the following pseudo-code is
followed, and terminates as soon as an attempted command exists:

always:command(...)            [in CLI Node]
find_node                      [in CLI Node]
if node is running:            [in CLI Node]
  running:command(...)         [in Running Node]
else:
  not_running:command(...)     [in CLI Node]
  if a new VM is started:
    started:command(...)       [in Newly Started Node]

Essentially, 'always' commands override everything, then only 'running'
commands when a node is running, OR 'not_running'. If requested 'started' is
used in a newly started system.

The idea is that the normal case is to either run commands with no context
(usually utility commands) or commands in the server process. Fallback
behavior is provided by "not running" cases, with "start" commands as the last
resort when no node is running. This "start" behavior is designed to make it
difficult to accidentally start multiple copies of a server when you instead
intend to have graceful startup for certain commands that are normally used
when a server is running.

Helpful Utilities
=================

To display output at the CLI node (correctly in any context), erlctl provides
a format/2 function (which is identical to io:format/2).

To set exit codes at the CLI node (correctly in any context), erlctl provides
a halt/1 function.

To cause the server to exit, erlctl provides a server_exit/0 function.

FIXME: System commands and Config Files aren't implemented yet.

To manage the configuration file, a "config" command is provided.  This 
command modifies the config file as a bag of key-value pairs.  No locking is
performed on the file, although it is atomically updated via symlink swaps on
systems that support it.

Config values can optionally specify a type.  Once a type is specified, an
update that changes the type of a value is not allowed (although the value 
may be deleted and recreated as separate entries).  The special type 'any'
allows any type of data to be set.  Also, all types support a 'null' value.
This can be used when initializing a configuration to provide type-safety
for configuration values while not specifying defaults.
